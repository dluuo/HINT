{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasetsforecast\n",
    "!pip install git+https://github.com/Nixtla/neuralforecast.git\n",
    "!pip install git+https://github.com/Nixtla/hierarchicalforecast.git\n",
    "!pip install --upgrade hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "#import wrangled datasets\n",
    "import sys\n",
    "sys.path = ['../'] + sys.path\n",
    "from hint.src.data.labour import Labour\n",
    "from hint.src.data.trafficS import TrafficS\n",
    "from hint.src.data.tourismS import TourismS\n",
    "from hint.src.data.tourismL import TourismL\n",
    "from hint.src.data.wiki2 import Wiki2\n",
    "\n",
    "# Hierarchical data\n",
    "from datasetsforecast.hierarchical import HierarchicalData\n",
    "\n",
    "# Reconciliation and evaluation\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.evaluation import HierarchicalEvaluation, scaled_crps\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut, MinTrace, ERM\n",
    "from hierarchicalforecast.utils import CodeTimer\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoNHITS, AutoNBEATS, AutoTFT, AutoTCN, AutoLSTM\n",
    "from neuralforecast.losses.pytorch import PMM, GMM, DistributionLoss\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "# 2. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_df_hier(Y_df, S):\n",
    "    Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S.index)\n",
    "    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "\n",
    "    return Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hierarchical_crps(data, Y, Y_hat, q_to_pred, model_name='current'):\n",
    "    hier_idxs   = data['hier_idxs']\n",
    "    crps_list = []\n",
    "    for i, idxs in enumerate(hier_idxs):\n",
    "        # Get the series specific to the hierarchical level\n",
    "        y     = Y[idxs, :]\n",
    "        y_hat = Y_hat[idxs, :, :]\n",
    "\n",
    "        crps  = scaled_crps(y, y_hat, q_to_pred)\n",
    "        crps_list.append(crps)\n",
    "\n",
    "    return crps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'Labour': Labour, \n",
    "            'Traffic': TrafficS,\n",
    "            'TourismSmall': TourismS,\n",
    "            'TourismLarge': TourismL,\n",
    "            'Wiki2': Wiki2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "# 3. Baseline CRPS Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baselines(dataset, models, scalar_type, verbose=False):\n",
    "    with CodeTimer('Read and Parse data   ', verbose):\n",
    "        data = datasets[dataset].load_process()\n",
    "        #import Y_df and remove future df NaNs\n",
    "        Y_df = data['temporal'][data['temporal']['y'].notna()]\n",
    "        Y_df['ds'] = pd.to_datetime(Y_df['ds'])  \n",
    "\n",
    "        #import static df\n",
    "        static_df = data['static']\n",
    "\n",
    "        #import S, tags, properties\n",
    "        S, tags, properties = data['S'], data['tags'], data['properties']\n",
    "        frequency, seasonality, horizon = properties['freq'], properties['seasonality'], properties['horizon']\n",
    "\n",
    "        # --Modeling parameters--\n",
    "        season_length = seasonality\n",
    "        horizon_len = horizon\n",
    "        freq = frequency\n",
    "        val_size = horizon\n",
    "        test_size = horizon\n",
    "        n_windows_cv = test_size//horizon\n",
    "        n_series = len(Y_df.unique_id.unique())\n",
    "        num_components = 10\n",
    "        num_samples = 2\n",
    "        input_size = 2*horizon\n",
    "\n",
    "        #create level and quantiles\n",
    "        level = np.arange(2, 100, 2)\n",
    "        qs = [[50-l/2, 50+l/2] for l in level] \n",
    "        quantiles = np.sort(np.concatenate(qs)/100)\n",
    "\n",
    "    with CodeTimer('Fit/Predict Model\t ', verbose):\n",
    "        \n",
    "        nhits_config = {\n",
    "            \"scaler_type\": tune.choice([scalar_type]),\n",
    "            \"hist_exog_list\": tune.choice([[f'y_[lag{seasonality}]']]),\n",
    "            \"learning_rate\": tune.choice([1e-3, 5e-3]),                                  # Initial Learning rate\n",
    "            \"max_steps\": tune.choice([500, 1000, 2000]),                                             # Number of SGD steps\n",
    "            \"input_size\": tune.choice([input_size]),                                          # input_size = multiplier * horizon\n",
    "            \"batch_size\": tune.choice([64]),                                                   # Number of series in windows\n",
    "            \"windows_batch_size\": tune.choice([256]),                                          # Number of windows in batch\n",
    "            \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [4, 2, 1]]),                         # MaxPool's Kernelsize\n",
    "            \"n_freq_downsample\": tune.choice([[12, 4, 1], [24, 12, 1], [4, 2, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
    "            \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),                 # 2 512-Layers per block for each stack\n",
    "            \"val_check_steps\": tune.choice([100]),                                             # Compute validation every 100 steps\n",
    "            \"early_stop_patience_steps\": tune.choice([5]),\n",
    "            \"num_lr_decays\": tune.choice([-1, 3]),\n",
    "            # \"random_seed\": tune.randint(1, 10), \n",
    "        }\n",
    "        nbeats_config = {\n",
    "            \"scaler_type\": tune.choice([scalar_type]),\n",
    "            \"hist_exog_list\": tune.choice([[f'y_[lag{seasonality}]']]),\n",
    "            \"learning_rate\": tune.choice([1e-3, 5e-3]),                                  # Initial Learning rate\n",
    "            \"max_steps\": tune.choice([500, 1000, 2000]),                                             # Number of SGD steps\n",
    "            \"input_size\": tune.choice([input_size]),                                          # input_size = multiplier * horizon\n",
    "            \"batch_size\": tune.choice([64]),                                                   # Number of series in windows\n",
    "            \"val_check_steps\": tune.choice([100]),                                             # Compute validation every 100 steps\n",
    "            \"early_stop_patience_steps\": tune.choice([5]),\n",
    "            \"num_lr_decays\": tune.choice([-1, 3]),\n",
    "            # \"random_seed\": tune.randint(1, 10),\n",
    "        }\n",
    "        tft_config = {\n",
    "            \"scaler_type\": tune.choice([scalar_type]),\n",
    "            \"hist_exog_list\": tune.choice([[f'y_[lag{seasonality}]']]),\n",
    "            \"learning_rate\": tune.choice([1e-3, 5e-3]),                                  # Initial Learning rate\n",
    "            \"max_steps\": tune.choice([500, 1000, 2000]),                                             # Number of SGD steps\n",
    "            \"input_size\": tune.choice([input_size]),                                          # input_size = multiplier * horizon\n",
    "            \"batch_size\": tune.choice([64]),                                                   # Number of series in windows\n",
    "            \"val_check_steps\": tune.choice([100]),                                             # Compute validation every 100 steps\n",
    "            \"early_stop_patience_steps\": tune.choice([5]),\n",
    "            \"num_lr_decays\": tune.choice([-1, 3]),\n",
    "            # \"random_seed\": tune.randint(1, 10),\n",
    "        }\n",
    "        tcn_config = {\n",
    "            \"scaler_type\": tune.choice([scalar_type]),\n",
    "            \"hist_exog_list\": tune.choice([[f'y_[lag{seasonality}]']]),\n",
    "            \"learning_rate\": tune.choice([1e-3, 5e-3]),                                  # Initial Learning rate\n",
    "            \"max_steps\": tune.choice([500, 1000, 2000]),                                             # Number of SGD steps\n",
    "            \"input_size\": tune.choice([input_size]),                                          # input_size = multiplier * horizon\n",
    "            \"batch_size\": tune.choice([64]),                                                   # Number of series in windows\n",
    "            \"val_check_steps\": tune.choice([100]),                                             # Compute validation every 100 steps\n",
    "            \"early_stop_patience_steps\": tune.choice([5]),\n",
    "            \"num_lr_decays\": tune.choice([-1, 3]),\n",
    "            # \"random_seed\": tune.randint(1, 10),\n",
    "        }\n",
    "        lstm_config = {\n",
    "            \"scaler_type\": tune.choice([scalar_type]),\n",
    "            \"hist_exog_list\": tune.choice([[f'y_[lag{seasonality}]']]),\n",
    "            \"learning_rate\": tune.choice([1e-3, 5e-3]),                                  # Initial Learning rate\n",
    "            \"max_steps\": tune.choice([500, 1000, 2000]),                                             # Number of SGD steps\n",
    "            \"input_size\": tune.choice([input_size]),                                          # input_size = multiplier * horizon\n",
    "            \"batch_size\": tune.choice([64]),                                                   # Number of series in windows\n",
    "            \"val_check_steps\": tune.choice([100]),                                             # Compute validation every 100 steps\n",
    "            \"early_stop_patience_steps\": tune.choice([5]),\n",
    "            \"num_lr_decays\": tune.choice([-1, 3]),\n",
    "            # \"random_seed\": tune.randint(1, 10),\n",
    "        }\n",
    "\n",
    "        model_dict = {\n",
    "            'NHITS' : AutoNHITS(config=nhits_config,\n",
    "                            loss=GMM(n_components=num_components, level=list(level)),\n",
    "                            h=horizon,\n",
    "                            search_alg=HyperOptSearch(),\n",
    "                            num_samples=num_samples),\n",
    "            'NBEATS' : AutoNBEATS(config=nbeats_config,\n",
    "                            loss=GMM(n_components=num_components, level=list(level)),\n",
    "                            h=horizon,\n",
    "                            search_alg=HyperOptSearch(),\n",
    "                            num_samples=num_samples),\n",
    "            'TFT' : AutoTFT(config=tft_config,\n",
    "                            loss=GMM(n_components=num_components, level=list(level)),\n",
    "                            h=horizon,\n",
    "                            search_alg=HyperOptSearch(),\n",
    "                            num_samples=num_samples),\n",
    "            'TCN' : AutoTCN(config=tft_config,\n",
    "                            loss=GMM(n_components=num_components, level=list(level)),\n",
    "                            h=horizon,\n",
    "                            search_alg=HyperOptSearch(),\n",
    "                            num_samples=num_samples),\n",
    "            'LSTM' : AutoLSTM(config=tft_config,\n",
    "                            loss=GMM(n_components=num_components, level=list(level)),\n",
    "                            h=horizon,\n",
    "                            search_alg=HyperOptSearch(),\n",
    "                            num_samples=num_samples)\n",
    "        }\n",
    "\n",
    "        model_list = [model_dict[model] for model in models]\n",
    "\n",
    "        #fit and predict\n",
    "        fcst = NeuralForecast(models=model_list, freq=frequency)\n",
    "\n",
    "        # Obtain forecast predictions\n",
    "        Y_hat_df = fcst.cross_validation(df=Y_df,\n",
    "                                         static_df=static_df,\n",
    "                                         n_windows=None,\n",
    "                                         val_size=val_size,\n",
    "                                         test_size=test_size)\n",
    "        \n",
    "        #sort raw predictions by uid to make sure they are hierarchically ordered\n",
    "        Y_hat_df = sort_df_hier(Y_hat_df, S)\n",
    "\n",
    "        # Group by cutoff date\n",
    "        Y_hat_group_df = Y_hat_df.groupby(['cutoff'])\n",
    "\n",
    "    with CodeTimer('Evaluate Models\t   ', verbose):\n",
    "\n",
    "        #Construct results dataframe\n",
    "        result = {'Dataset': [dataset],\n",
    "                'Level': ['Overall'],\n",
    "                'Scalar Type':scalar_type}\n",
    "\n",
    "        for model in models:\n",
    "            #turn Y_hat_df from grouped dataframe to normal dataframe and sort by cutoff/uid\n",
    "            Y_hat_sort_df = Y_hat_group_df.apply(lambda x: x)\n",
    "            Y_hat_sort_df = Y_hat_sort_df.sort_values(by=['cutoff','unique_id'])\n",
    "\n",
    "            #extract just y predictions to get Y_test\n",
    "            Y_test_df = Y_hat_sort_df['y']\n",
    "\n",
    "            #drop all columns that aren't quantile predictions\n",
    "            #keep only columns starting with {model}-lo- or {model}-hi-\n",
    "            pattern = re.compile(f\"^Auto{model}-(lo|hi)-\")\n",
    "            Y_hat_quantiles_df = Y_hat_sort_df.filter(regex=pattern)\n",
    "\n",
    "            #convert dataframes to numpy and reshape\n",
    "            Y_test_np = Y_test_df.values.reshape(n_windows_cv, n_series, horizon_len, 1)\n",
    "            Y_hat_np = Y_hat_quantiles_df.values.reshape(n_windows_cv, n_series, horizon_len, -1)\n",
    "\n",
    "            crps_list = [get_hierarchical_crps(data, \n",
    "                                                Y=Y_test_np[window,:,:,0], \n",
    "                                                Y_hat=Y_hat_np[window,:,:,:], \n",
    "                                                q_to_pred=quantiles) for window in range(n_windows_cv)]\n",
    "\n",
    "            #get just overall crps\n",
    "            crps_list_np = np.array(crps_list[0])\n",
    "            crps_avg_np = np.average(crps_list_np, axis=0)\n",
    "\n",
    "            \n",
    "\n",
    "            result[model] = crps_avg_np\n",
    "\n",
    "        result_df = pd.DataFrame(result)\n",
    "\n",
    "        return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "# 4. Run Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "verbose = True\n",
    "result_df_list = []\n",
    "models = ['NHITS', 'NBEATS', 'TFT', 'TCN', 'LSTM']\n",
    "dataset = 'Wiki2'\n",
    "scalar_types = ['minmax', 'standard', 'robust', None]\n",
    "for scalar_type in scalar_types:\n",
    "    result_df = run_baselines(dataset=dataset, models=models, scalar_type=scalar_type, verbose=verbose)\n",
    "    result_df_list.append(result_df)\n",
    "\n",
    "overall_result_df = pd.concat(result_df_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_result_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-5\"></a>\n",
    "# 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bars = len(scalar_types)\n",
    "num_models = len(models)\n",
    "barWidth = 0.20\n",
    "bar_colors = ['#235796', '#20425B', '#78ACA8', '#7B3841']\n",
    "architectures = models\n",
    "scalar_types = ['minmax', 'standard', 'robust', 'none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set heights of bars\n",
    "bar_heights = [overall_result_df[models].values[i] for i in range(num_bars)]\n",
    "\n",
    "# Set position of bars on X axis\n",
    "r1 = np.arange(num_models)\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "bar_pos = [r1,r2,r3,r4]\n",
    " \n",
    "# Make the plot\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.grid(zorder=3)\n",
    "ax.set_axisbelow(True)\n",
    "for i in range(num_bars):\n",
    "    ax.bar(x=bar_pos[i], height=bar_heights[i], color=bar_colors[i],\n",
    "           width=barWidth, edgecolor='white', label=f'{scalar_types[i]}')\n",
    "    \n",
    "#set the y-axis to be log scale\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Architecture', fontsize=14)\n",
    "plt.ylabel('log(sCRPS)', fontsize=14)\n",
    "plt.xticks(ticks=[r + barWidth for r in range(num_models)], \n",
    "           labels=architectures, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    " \n",
    "\n",
    "# Create legend & Show graphic\n",
    "# legend = plt.legend(bbox_to_anchor=(0.25, 1.85, 1., .12), loc='upper left',\n",
    "#            ncol=1, borderaxespad=0, fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f'./data/AblationScale{dataset}.pdf', bbox_inches='tight', pad_inches = 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "231ab5be553983f5d18761458fd09479b8ea8fd2f7d5e70fe8cc57d3219e6899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
